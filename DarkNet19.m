%% DarkNet19.m
% Tianyi Wang
% 1/18/2022

%% Create and Train a Deep Learning Model
% Script for creating and training a deep learning network with the following 
% properties:
%%
% 
%  Number of layers: 64
%  Number of connections: 63
%  Training setup file: /Users/Tianyi/Documents/GRAD/Rajalab/AIMproject/NNFiles2.2/params_2022_01_18__17_52_41.mat
%
%% 
% Run this script to create the network layers, import training and validation 
% data, and train the network. The network layers are stored in the workspace 
% variable |layers|. The trained network is stored in the workspace variable |net|.
% 
% To learn more, see <matlab:helpview('deeplearning','generate_matlab_code') 
% Generate MATLAB Code From Deep Network Designer>.
% 
% Auto-generated by MATLAB on 18-Jan-2022 17:53:09

%% Set Up
% Change the varbules here to afect how the net works
tp = 0.7;          % Persent of images to be for training(0 to 1)
Fnum = 400;         % Number of feature that the net will look at
LayerWith = 800;    % Number of nodes in the layers

%% Load Initial Parameters
% Load parameters for network initialization. For transfer learning, the network 
% initialization parameters are the parameters of the initial pretrained network.

trainingSetup = load("/Users/Tianyi/Documents/GRAD/Rajalab/AIMproject/NNFiles2.2/params_2022_01_18__17_52_41.mat");

% %% Laods Images
% imds = imageDatastore('DisData_2000images_15sets','IncludeSubfolders',true,'LabelSource','foldernames');
% imds.ReadFcn = @readAndResizeImages;
% imdsM = imageDatastore('Data_2000images_15sets','IncludeSubfolders',true,'LabelSource','foldernames');
% imdsM.ReadFcn = @readAndResizeImages;
% tbl = countEachLabel(imds); % Shows name and # of samples
% categories = tbl.Label;
% 
% visImds = splitEachLabel(imds,1,'randomize');

% %% Import Data
% % Import training and validation data.
% 
imdsTrain = imageDatastore("/Users/Tianyi/Documents/GRAD/Rajalab/AIMproject/NNFiles2.2/DisData","IncludeSubfolders",true,"LabelSource","foldernames");
imdsM = imageDatastore("/Users/Tianyi/Documents/GRAD/Rajalab/AIMproject/NNFiles2.2/Data","IncludeSubfolders",true,"LabelSource","foldernames");
% [imdsTrain, imdsValidation] = splitEachLabel(imdsTrain,0.7);
% 
% % Resize the images to match the network input layer.
% augimdsTrain = augmentedImageDatastore([256 256 3],imdsTrain);
% augimdsValidation = augmentedImageDatastore([256 256 3],imdsValidation);

%% Make traing/test sets
% Data split
[imdsTrain, imdsValidation] = imdsTrain.splitEachLabel(tp,1-tp,'randomize',true);
% Resize the images to match the network input layer.
augimdsTrain = augmentedImageDatastore([256 256 3],imdsTrain);
augimdsValidation = augmentedImageDatastore([256 256 3],imdsValidation);
% Makes bag of features
bag1 = bagOfFeatures(imageSet(imdsM.Files),...
'VocabularySize',Fnum,'PointSelection','Detector');
% Makes Training data
scenedata1 = double(encode(bag1,imageSet(imdsTrain.Files)));
% Makes Test data
scenedata2 = double(encode(bag1,imageSet(imdsValidation.Files)));
%% Set Training Options
% Specify options to use when training.

opts = trainingOptions("sgdm",...
    "ExecutionEnvironment","auto",...
    "InitialLearnRate",0.001,...
    "MiniBatchSize",137,...
    "Shuffle","every-epoch",...
    "ValidationFrequency",2,...
    "Plots","training-progress",...
    "ValidationData",augimdsValidation);
%% Create Array of Layers

layers = [
    imageInputLayer([256 256 3],"Name","input","Normalization","rescale-zero-one","Max",trainingSetup.input.Max,"Min",trainingSetup.input.Min)
    convolution2dLayer([3 3],32,"Name","conv1","Padding","same","Bias",trainingSetup.conv1.Bias,"Weights",trainingSetup.conv1.Weights)
    batchNormalizationLayer("Name","batchnorm1","Offset",trainingSetup.batchnorm1.Offset,"Scale",trainingSetup.batchnorm1.Scale,"TrainedMean",trainingSetup.batchnorm1.TrainedMean,"TrainedVariance",trainingSetup.batchnorm1.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky1")
    maxPooling2dLayer([2 2],"Name","pool1","Stride",[2 2])
    convolution2dLayer([3 3],64,"Name","conv2","Padding","same","Bias",trainingSetup.conv2.Bias,"Weights",trainingSetup.conv2.Weights)
    batchNormalizationLayer("Name","batchnorm2","Offset",trainingSetup.batchnorm2.Offset,"Scale",trainingSetup.batchnorm2.Scale,"TrainedMean",trainingSetup.batchnorm2.TrainedMean,"TrainedVariance",trainingSetup.batchnorm2.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky2")
    maxPooling2dLayer([2 2],"Name","pool2","Stride",[2 2])
    convolution2dLayer([3 3],128,"Name","conv3","Padding","same","Bias",trainingSetup.conv3.Bias,"Weights",trainingSetup.conv3.Weights)
    batchNormalizationLayer("Name","batchnorm3","Offset",trainingSetup.batchnorm3.Offset,"Scale",trainingSetup.batchnorm3.Scale,"TrainedMean",trainingSetup.batchnorm3.TrainedMean,"TrainedVariance",trainingSetup.batchnorm3.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky3")
    convolution2dLayer([1 1],64,"Name","conv4","Padding","same","Bias",trainingSetup.conv4.Bias,"Weights",trainingSetup.conv4.Weights)
    batchNormalizationLayer("Name","batchnorm4","Offset",trainingSetup.batchnorm4.Offset,"Scale",trainingSetup.batchnorm4.Scale,"TrainedMean",trainingSetup.batchnorm4.TrainedMean,"TrainedVariance",trainingSetup.batchnorm4.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky4")
    convolution2dLayer([3 3],128,"Name","conv5","Padding","same","Bias",trainingSetup.conv5.Bias,"Weights",trainingSetup.conv5.Weights)
    batchNormalizationLayer("Name","batchnorm5","Offset",trainingSetup.batchnorm5.Offset,"Scale",trainingSetup.batchnorm5.Scale,"TrainedMean",trainingSetup.batchnorm5.TrainedMean,"TrainedVariance",trainingSetup.batchnorm5.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky5")
    maxPooling2dLayer([2 2],"Name","pool3","Stride",[2 2])
    convolution2dLayer([3 3],256,"Name","conv6","Padding","same","Bias",trainingSetup.conv6.Bias,"Weights",trainingSetup.conv6.Weights)
    batchNormalizationLayer("Name","batchnorm6","Offset",trainingSetup.batchnorm6.Offset,"Scale",trainingSetup.batchnorm6.Scale,"TrainedMean",trainingSetup.batchnorm6.TrainedMean,"TrainedVariance",trainingSetup.batchnorm6.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky6")
    convolution2dLayer([1 1],128,"Name","conv7","Padding","same","Bias",trainingSetup.conv7.Bias,"Weights",trainingSetup.conv7.Weights)
    batchNormalizationLayer("Name","batchnorm7","Offset",trainingSetup.batchnorm7.Offset,"Scale",trainingSetup.batchnorm7.Scale,"TrainedMean",trainingSetup.batchnorm7.TrainedMean,"TrainedVariance",trainingSetup.batchnorm7.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky7")
    convolution2dLayer([3 3],256,"Name","conv8","Padding","same","Bias",trainingSetup.conv8.Bias,"Weights",trainingSetup.conv8.Weights)
    batchNormalizationLayer("Name","batchnorm8","Offset",trainingSetup.batchnorm8.Offset,"Scale",trainingSetup.batchnorm8.Scale,"TrainedMean",trainingSetup.batchnorm8.TrainedMean,"TrainedVariance",trainingSetup.batchnorm8.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky8")
    maxPooling2dLayer([2 2],"Name","pool4","Stride",[2 2])
    convolution2dLayer([3 3],512,"Name","conv9","Padding","same","Bias",trainingSetup.conv9.Bias,"Weights",trainingSetup.conv9.Weights)
    batchNormalizationLayer("Name","batchnorm9","Offset",trainingSetup.batchnorm9.Offset,"Scale",trainingSetup.batchnorm9.Scale,"TrainedMean",trainingSetup.batchnorm9.TrainedMean,"TrainedVariance",trainingSetup.batchnorm9.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky9")
    convolution2dLayer([1 1],256,"Name","conv10","Padding","same","Bias",trainingSetup.conv10.Bias,"Weights",trainingSetup.conv10.Weights)
    batchNormalizationLayer("Name","batchnorm10","Offset",trainingSetup.batchnorm10.Offset,"Scale",trainingSetup.batchnorm10.Scale,"TrainedMean",trainingSetup.batchnorm10.TrainedMean,"TrainedVariance",trainingSetup.batchnorm10.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky10")
    convolution2dLayer([3 3],512,"Name","conv11","Padding","same","Bias",trainingSetup.conv11.Bias,"Weights",trainingSetup.conv11.Weights)
    batchNormalizationLayer("Name","batchnorm11","Offset",trainingSetup.batchnorm11.Offset,"Scale",trainingSetup.batchnorm11.Scale,"TrainedMean",trainingSetup.batchnorm11.TrainedMean,"TrainedVariance",trainingSetup.batchnorm11.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky11")
    convolution2dLayer([1 1],256,"Name","conv12","Padding","same","Bias",trainingSetup.conv12.Bias,"Weights",trainingSetup.conv12.Weights)
    batchNormalizationLayer("Name","batchnorm12","Offset",trainingSetup.batchnorm12.Offset,"Scale",trainingSetup.batchnorm12.Scale,"TrainedMean",trainingSetup.batchnorm12.TrainedMean,"TrainedVariance",trainingSetup.batchnorm12.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky12")
    convolution2dLayer([3 3],512,"Name","conv13","Padding","same","Bias",trainingSetup.conv13.Bias,"Weights",trainingSetup.conv13.Weights)
    batchNormalizationLayer("Name","batchnorm13","Offset",trainingSetup.batchnorm13.Offset,"Scale",trainingSetup.batchnorm13.Scale,"TrainedMean",trainingSetup.batchnorm13.TrainedMean,"TrainedVariance",trainingSetup.batchnorm13.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky13")
    maxPooling2dLayer([2 2],"Name","pool5","Stride",[2 2])
    convolution2dLayer([3 3],1024,"Name","conv14","Padding","same","Bias",trainingSetup.conv14.Bias,"Weights",trainingSetup.conv14.Weights)
    batchNormalizationLayer("Name","batchnorm14","Offset",trainingSetup.batchnorm14.Offset,"Scale",trainingSetup.batchnorm14.Scale,"TrainedMean",trainingSetup.batchnorm14.TrainedMean,"TrainedVariance",trainingSetup.batchnorm14.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky14")
    convolution2dLayer([1 1],512,"Name","conv15","Padding","same","Bias",trainingSetup.conv15.Bias,"Weights",trainingSetup.conv15.Weights)
    batchNormalizationLayer("Name","batchnorm15","Offset",trainingSetup.batchnorm15.Offset,"Scale",trainingSetup.batchnorm15.Scale,"TrainedMean",trainingSetup.batchnorm15.TrainedMean,"TrainedVariance",trainingSetup.batchnorm15.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky15")
    convolution2dLayer([3 3],1024,"Name","conv16","Padding","same","Bias",trainingSetup.conv16.Bias,"Weights",trainingSetup.conv16.Weights)
    batchNormalizationLayer("Name","batchnorm16","Offset",trainingSetup.batchnorm16.Offset,"Scale",trainingSetup.batchnorm16.Scale,"TrainedMean",trainingSetup.batchnorm16.TrainedMean,"TrainedVariance",trainingSetup.batchnorm16.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky16")
    convolution2dLayer([1 1],512,"Name","conv17","Padding","same","Bias",trainingSetup.conv17.Bias,"Weights",trainingSetup.conv17.Weights)
    batchNormalizationLayer("Name","batchnorm17","Offset",trainingSetup.batchnorm17.Offset,"Scale",trainingSetup.batchnorm17.Scale,"TrainedMean",trainingSetup.batchnorm17.TrainedMean,"TrainedVariance",trainingSetup.batchnorm17.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky17")
    convolution2dLayer([3 3],1024,"Name","conv18","Padding","same","Bias",trainingSetup.conv18.Bias,"Weights",trainingSetup.conv18.Weights)
    batchNormalizationLayer("Name","batchnorm18","Offset",trainingSetup.batchnorm18.Offset,"Scale",trainingSetup.batchnorm18.Scale,"TrainedMean",trainingSetup.batchnorm18.TrainedMean,"TrainedVariance",trainingSetup.batchnorm18.TrainedVariance)
    leakyReluLayer(0.1,"Name","leaky18")
    convolution2dLayer([1 1],10,"Name","conv","Padding","same")
    globalAveragePooling2dLayer("Name","avg1")
    softmaxLayer("Name","softmax")
    classificationLayer("Name","classoutput")];
%% Train Network
% Train the network using the specified options and training data.

[net, traininfo] = trainNetwork(augimdsTrain,layers,opts);

%% Test Net
%--Tresting data--%
% Uses our model to perdict data
[POC] = predict(net, augimdsValidation); 
% Get top 5 choices
TopPer = TopPerdict(POC,net.ClassNames);
% Checks for the corectness rate of first then top 5
actualSceneTypeTest = test_set.Labels;
correctPredictions = (POC == actualSceneTypeTest);
validationAccuracy = sum(correctPredictions)/length(POC);
TesAcFirst = validationAccuracy*100;
correctPredictions = (TopPer(:,1) == actualSceneTypeTest...
    |TopPer(:,2) == actualSceneTypeTest...
    |TopPer(:,3) == actualSceneTypeTest...
    |TopPer(:,4) == actualSceneTypeTest...
    |TopPer(:,5) == actualSceneTypeTest);
validationAccuracy = sum(correctPredictions)/length(POC);
TesAcFirst5 = validationAccuracy*100; 
% Outputs accuracy rates for net with nd without top 5 outputs
fprintf('-----For F = %f, Layers = %f----- \n', Fnum, LayerWith)
fprintf('AcTest5 = %f\n', TesAcFirst5);
fprintf('AcTest = %f\n', TesAcFirst);
fprintf(' \n');

%% Save Net
save('DarkNet19.mat','net')